{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Equilibrium (DEQ) algorithms for image deblurring\n\nThis a toy example to show you how to use DEQ to solve a deblurring problem. \nNote that this is a small dataset for training. For optimal results, use a larger dataset.\nFor visualizing the training, you can use Weight&Bias (wandb) by setting ``wandb_vis=True``.\n\nFor now DEQ is only possible with PGD, HQS and GD optimization algorithms. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import DataLoader\nfrom deepinv.models import DnCNN\nfrom deepinv.optim.data_fidelity import L2\nfrom deepinv.optim.prior import PnP\nfrom deepinv.unfolded import DEQ_builder\nfrom deepinv.training_utils import train, test\nfrom torchvision import transforms\nfrom deepinv.utils.demo import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup paths for data loading and results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\".\")\nORIGINAL_DATA_DIR = BASE_DIR / \"datasets\"\nDATA_DIR = BASE_DIR / \"measurements\"\nRESULTS_DIR = BASE_DIR / \"results\"\nCKPT_DIR = BASE_DIR / \"ckpts\"\n\n# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load base image datasets and degradation operators.\nIn this example, we use the CBSD500 dataset and the Set3C dataset for testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img_size = 32\nn_channels = 3  # 3 for color images, 1 for gray-scale images\noperation = \"deblurring\"\n# For simplicity, we use a small dataset for training.\n# To be replaced for optimal results. For example, you can use the larger \"drunet\" dataset.\ntrain_dataset_name = \"CBSD500\"\ntest_dataset_name = \"set3c\"\n# Generate training and evaluation datasets in HDF5 folders and load them.\ntest_transform = transforms.Compose(\n    [transforms.CenterCrop(img_size), transforms.ToTensor()]\n)\ntrain_transform = transforms.Compose(\n    [transforms.RandomCrop(img_size), transforms.ToTensor()]\n)\ntrain_base_dataset = load_dataset(\n    train_dataset_name, ORIGINAL_DATA_DIR, transform=train_transform\n)\ntest_base_dataset = load_dataset(\n    test_dataset_name, ORIGINAL_DATA_DIR, transform=test_transform\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a dataset of low resolution images and load it.\nWe use the Downsampling class from the physics module to generate a dataset of low resolution images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use parallel dataloader if using a GPU to fasten training, otherwise, as all computes are on CPU, use synchronous\n# dataloading.\nnum_workers = 4 if torch.cuda.is_available() else 0\n\n# Degradation parameters\nnoise_level_img = 0.03\n\n# Generate the gaussian blur downsampling operator.\nphysics = dinv.physics.BlurFFT(\n    img_size=(n_channels, img_size, img_size),\n    filter=dinv.physics.blur.gaussian_blur(),\n    device=device,\n    noise_model=dinv.physics.GaussianNoise(sigma=noise_level_img),\n)\nmy_dataset_name = \"demo_DEQ\"\nn_images_max = (\n    1000 if torch.cuda.is_available() else 10\n)  # maximal number of images used for training\nmeasurement_dir = DATA_DIR / train_dataset_name / operation\ngenerated_datasets_path = dinv.datasets.generate_dataset(\n    train_dataset=train_base_dataset,\n    test_dataset=test_base_dataset,\n    physics=physics,\n    device=device,\n    save_dir=measurement_dir,\n    train_datapoints=n_images_max,\n    num_workers=num_workers,\n    dataset_filename=str(my_dataset_name),\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=True)\ntest_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the  DEQ algorithm.\nWe use the helper function :meth:`deepinv.unfolded.DEQ_builder` to defined the DEQ architecture.\nThe chosen algorithm is here HQS (Half Quadratic Splitting).\nNote for DEQ, the prior and regularization parameters should be common for all iterations\nto keep a constant fixed-point operator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Select the data fidelity term\ndata_fidelity = L2()\n\n# Set up the trainable denoising prior\ndenoiser = DnCNN(\n    in_channels=3, out_channels=3, depth=7, device=device, pretrained=None, train=True\n)\n\n# Here the prior model is common for all iterations\nprior = PnP(denoiser=denoiser)\n\n# Unrolled optimization algorithm parameters\nmax_iter = 20 if torch.cuda.is_available() else 10\nlamb = 1.0  # Initial value for the regularization parameter.\nstepsize = 1.0  # Initial value for the stepsize. A single stepsize is common for each iterations.\nsigma_denoiser = 0.03  # Initial value for the denoiser parameter. A single value is common for each iterations.\nanderson_acceleration_forward = True  # use Anderson acceleration for the forward pass.\nanderson_acceleration_backward = (\n    True  # use Anderson acceleration for the backward pass.\n)\nanderson_history_size = (\n    5 if torch.cuda.is_available() else 3\n)  # history size for Anderson acceleration.\n\nparams_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary\n    \"stepsize\": stepsize,\n    \"g_param\": sigma_denoiser,\n    \"lambda\": lamb,\n}\ntrainable_params = [\n    \"lambda\",\n    \"stepsize\",\n    \"g_param\",\n]  # define which parameters from 'params_algo' are trainable\n\n# Define the unfolded trainable model.\nmodel = DEQ_builder(\n    iteration=\"HQS\",  # For now DEQ is only possible with PGD, HQS and GD optimization algorithms.\n    params_algo=params_algo.copy(),\n    trainable_params=trainable_params,\n    data_fidelity=data_fidelity,\n    max_iter=max_iter,\n    prior=prior,\n    anderson_acceleration=anderson_acceleration_forward,\n    anderson_acceleration_backward=anderson_acceleration_backward,\n    history_size_backward=anderson_history_size,\n    history_size=anderson_history_size,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the training parameters.\nWe use the Adam optimizer and the StepLR scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# training parameters\nepochs = 10\nlearning_rate = 5e-4\ntrain_batch_size = 32 if torch.cuda.is_available() else 1\ntest_batch_size = 3\n\n# choose optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8))\n\n# choose supervised training loss\nlosses = [dinv.loss.SupLoss(metric=dinv.metric.mse())]\n\n# Logging parameters\nverbose = True\nwandb_vis = False  # plot curves and images in Weight&Bias\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True\n)\ntest_dataloader = DataLoader(\n    test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the network\nWe train the network using the library's train function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train(\n    model=model,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    epochs=epochs,\n    scheduler=scheduler,\n    losses=losses,\n    physics=physics,\n    optimizer=optimizer,\n    device=device,\n    save_path=str(CKPT_DIR / operation),\n    verbose=verbose,\n    wandb_vis=wandb_vis,  # training visualization can be done in Weight&Bias\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the network\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "method = \"DEQ_HQS\"\nsave_folder = RESULTS_DIR / method / operation\nwandb_vis = False  # plot curves and images in Weight&Bias.\nplot_images = True  # plot images. Images are saved in save_folder.\n\ntest(\n    model=model,\n    test_dataloader=test_dataloader,\n    physics=physics,\n    device=device,\n    plot_metrics=True,\n    plot_images=plot_images,\n    save_folder=save_folder,\n    verbose=verbose,\n    wandb_vis=wandb_vis,\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}