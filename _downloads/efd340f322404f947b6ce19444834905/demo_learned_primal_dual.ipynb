{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learned Primal-Dual algorithm for CT scan.\nImplementation of the Unfolded Primal-Dual algorithm from\n\nAdler, Jonas, and Ozan \u00d6ktem. \n\"Learned primal-dual reconstruction.\" \nIEEE transactions on medical imaging 37.6 (2018): 1322-1332.\n\nwhere both the data fidelity and the prior are learned modules, distinct for each iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import DataLoader\nfrom deepinv.unfolded import unfolded_builder\nfrom deepinv.training_utils import train, test\nfrom torchvision import transforms\nfrom deepinv.utils.demo import load_dataset\nfrom deepinv.optim.optim_iterators import CPIteration, fStep, gStep\nfrom deepinv.models.PDNet import PrimalBlock, DualBlock\nfrom deepinv.optim import Prior, DataFidelity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup paths for data loading and results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\".\")\nORIGINAL_DATA_DIR = BASE_DIR / \"datasets\"\nDATA_DIR = BASE_DIR / \"measurements\"\nRESULTS_DIR = BASE_DIR / \"results\"\nCKPT_DIR = BASE_DIR / \"ckpts\"\n\n# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load base image datasets and degradation operators.\nIn this example, we use the CBSD500 dataset for training and the Set3C dataset for testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img_size = 64 if torch.cuda.is_available() else 32\nn_channels = 3  # 3 for color images, 1 for gray-scale images\noperation = \"CT\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a dataset of low resolution images and load it.\nWe use the Downsampling class from the physics module to generate a dataset of low resolution images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For simplicity, we use a small dataset for training.\n# To be replaced for optimal results. For example, you can use the larger \"drunet\" dataset.\ntrain_dataset_name = \"CBSD500\"\ntest_dataset_name = \"set3c\"\n# Specify the  train and test transforms to be applied to the input images.\ntest_transform = transforms.Compose(\n    [transforms.CenterCrop(img_size), transforms.ToTensor()]\n)\ntrain_transform = transforms.Compose(\n    [transforms.RandomCrop(img_size), transforms.ToTensor()]\n)\n# Define the base train and test datasets of clean images.\ntrain_base_dataset = load_dataset(\n    train_dataset_name, ORIGINAL_DATA_DIR, transform=train_transform\n)\ntest_base_dataset = load_dataset(\n    test_dataset_name, ORIGINAL_DATA_DIR, transform=test_transform\n)\n\n# Use parallel dataloader if using a GPU to fasten training, otherwise, as all computes are on CPU, use synchronous\n# dataloading.\nnum_workers = 4 if torch.cuda.is_available() else 0\n\n# Degradation parameters\nfactor = 2\nnoise_level_img = 0.03\n\n# Generate the gaussian blur downsampling operator.\nphysics = dinv.physics.Downsampling(\n    img_size=(n_channels, img_size, img_size),\n    factor=factor,\n    mode=\"gauss\",\n    device=device,\n    noise_model=dinv.physics.GaussianNoise(sigma=noise_level_img),\n)\nmy_dataset_name = \"demo_unfolded_sr\"\nn_images_max = (\n    1000 if torch.cuda.is_available() else 10\n)  # maximal number of images used for training\nmeasurement_dir = DATA_DIR / train_dataset_name / operation\ngenerated_datasets_path = dinv.datasets.generate_dataset(\n    train_dataset=train_base_dataset,\n    test_dataset=test_base_dataset,\n    physics=physics,\n    device=device,\n    save_dir=measurement_dir,\n    train_datapoints=n_images_max,\n    num_workers=num_workers,\n    dataset_filename=str(my_dataset_name),\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=True)\ntest_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a custom iterator for the PDNet learned primal-dual algorithm.\nThe iterator is a subclass of the Chambolle-Pock iterator :meth:`deepinv.optim.optim_iterators.PDIteration`.\nIn PDNet, the primal (gStep) and dual (fStep) updates are directly replaced by neural networks.\nWe thus redefine the fStep and gStep classes as simple proximal operators of the data fidelity and prior, respectively.\nAfterwards, both the data fidelity and the prior proximal operators are defined as trainable models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PDNetIteration(CPIteration):\n    r\"\"\"Single iteration of learned primal dual.\n    We only redefine the fStep and gStep classes.\n    The forward method is inherited from the CPIteration class.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.g_step = gStepPDNet(**kwargs)\n        self.f_step = fStepPDNet(**kwargs)\n\n\nclass fStepPDNet(fStep):\n    r\"\"\"\n    Dual update of the PDNet algorithm.\n    We write it as a proximal operator of the data fidelity term.\n    This proximal mapping is to be replaced by a trainable model.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def forward(self, x, w, cur_data_fidelity, y, *args):\n        r\"\"\"\n        :param torch.Tensor x: Current first variable :math:`u`.\n        :param torch.Tensor w: Current second variable :math:`A z`.\n        :param deepinv.optim.data_fidelity cur_data_fidelity: Instance of the DataFidelity class defining the current data fidelity term.\n        :param torch.Tensor y: Input data.\n        \"\"\"\n        return cur_data_fidelity.prox(x, w, y)\n\n\nclass gStepPDNet(gStep):\n    r\"\"\"\n    Primal update of the PDNet algorithm.\n    We write it as a proximal operator of the prior term.\n    This proximal mapping is to be replaced by a trainable model.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def forward(self, x, w, cur_prior, *args):\n        r\"\"\"\n        :param torch.Tensor x: Current first variable :math:`x`.\n        :param torch.Tensor w: Current second variable :math:`A^\\top u`.\n        :param deepinv.optim.prior cur_prior: Instance of the Prior class defining the current prior.\n        \"\"\"\n        return cur_prior.prox(x, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the trainable prior and data fidelity terms.\nPrior and data-fidelity are respectively defined as subclass of :meth:`deepinv.optim.Prior` and :meth:`deepinv.optim.DataFidelity`.\nTheir proximal operators are replaced by trainable models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PDNetPrior(Prior):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n\n    def prox(self, x, w):\n        return self.model(x, w)\n\n\nclass PDNetDataFid(DataFidelity):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n\n    def prox(self, x, w, y):\n        return self.model(x, w, y)\n\n\n# Unrolled optimization algorithm parameters\nmax_iter = 5  # number of unfolded layers\n\n# Set up the data fidelity term. Each layer has its own data fidelity module.\ndata_fidelity = [\n    PDNetDataFid(model=DualBlock(in_channels=9).to(device)) for i in range(max_iter)\n]\n\n# Set up the trainable prior. Each layer has its own prior module.\nprior = [\n    PDNetPrior(model=PrimalBlock(in_channels=6).to(device)) for i in range(max_iter)\n]\n\n\n# Logging parameters\nverbose = True\nwandb_vis = False  # plot curves and images in Weight&Bias\n\n\ndef custom_init(y, physics):\n    z0 = physics.A_adjoint(y)\n    x0 = physics.A_adjoint(y)\n    u0 = y\n    return {\"est\": (x0, z0, u0)}\n\n\n# Define the unfolded trainable model.\nmodel = unfolded_builder(\n    iteration=PDNetIteration(),\n    params_algo={\"K\": physics.A, \"K_adjoint\": physics.A_adjoint, \"beta\": 1.0},\n    data_fidelity=data_fidelity,\n    prior=prior,\n    max_iter=max_iter,\n    custom_init=custom_init,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the training parameters.\nWe use the Adam optimizer and the StepLR scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# training parameters\nepochs = 10 if torch.cuda.is_available() else 2\nlearning_rate = 5e-4\ntrain_batch_size = 32 if torch.cuda.is_available() else 1\ntest_batch_size = 3\n\n# choose optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8))\n\n# choose supervised training loss\nlosses = [dinv.loss.SupLoss(metric=dinv.metric.mse())]\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True\n)\ntest_dataloader = DataLoader(\n    test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the network\nWe train the network using the library's train function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train(\n    model=model,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    epochs=epochs,\n    scheduler=scheduler,\n    losses=losses,\n    physics=physics,\n    optimizer=optimizer,\n    device=device,\n    save_path=str(CKPT_DIR / operation),\n    verbose=verbose,\n    wandb_vis=wandb_vis,  # training visualization can be done in Weight&Bias\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the network\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "method = \"learned primal-dual\"\nsave_folder = RESULTS_DIR / method / operation\nwandb_vis = False  # plot curves and images in Weight&Bias.\nplot_images = True  # plot images. Images are saved in save_folder.\nplot_metrics = True  # compute performance and convergence metrics along the algorithm, curved saved in RESULTS_DIR\n\ntest(\n    model=model,\n    test_dataloader=test_dataloader,\n    physics=physics,\n    device=device,\n    plot_images=plot_images,\n    save_folder=save_folder,\n    verbose=verbose,\n    plot_metrics=plot_metrics,\n    wandb_vis=wandb_vis,  # test visualization can be done in Weight&Bias\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}