{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-supervised learning with Equivariant Imaging for MRI.\n\nThis example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.\n\nThe equivariant imaging loss is presented in [\"Equivariant Imaging: Learning Beyond the Range Space\"](http://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import deepinv as dinv\nfrom torch.utils.data import DataLoader\nimport torch\nfrom pathlib import Path\nfrom torchvision import transforms\nfrom deepinv.optim.prior import PnP\nfrom deepinv.utils.demo import load_dataset, load_degradation\nfrom deepinv.training_utils import train, test\nfrom deepinv.models.denoiser import online_weights_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup paths for data loading and results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\".\")\nORIGINAL_DATA_DIR = BASE_DIR / \"datasets\"\nDATA_DIR = BASE_DIR / \"measurements\"\nRESULTS_DIR = BASE_DIR / \"results\"\nDEG_DIR = BASE_DIR / \"degradations\"\nCKPT_DIR = BASE_DIR / \"ckpts\"\n\n# Set the global random seed from pytorch to ensure reproducibility of the example.\ntorch.manual_seed(0)\n\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load base image datasets and degradation operators.\nIn this example, we use a subset of the single-coil [FastMRI dataset](https://fastmri.org/)\nas the base image dataset. It consists of 973 knee images of size 320x320.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We reduce to the size to 128x128 for faster training in the demo.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "operation = \"MRI\"\ntrain_dataset_name = \"fastmri_knee_singlecoil\"\nimg_size = 128\n\ntransform = transforms.Compose([transforms.Resize(img_size)])\n\ntrain_dataset = load_dataset(\n    train_dataset_name, ORIGINAL_DATA_DIR, transform, train=True\n)\ntest_dataset = load_dataset(\n    train_dataset_name, ORIGINAL_DATA_DIR, transform, train=False\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a dataset of knee images and load it.\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = load_degradation(\"mri_mask_128x128.npy\", ORIGINAL_DATA_DIR)\n\n# defined physics\nphysics = dinv.physics.MRI(mask=mask, device=device)\n\n# Use parallel dataloader if using a GPU to fasten training,\n# otherwise, as all computes are on CPU, use synchronous data loading.\nnum_workers = 4 if torch.cuda.is_available() else 0\nn_images_max = (\n    900 if torch.cuda.is_available() else 5\n)  # number of images used for training\n# (the dataset has up to 973 images, however here we use only 900)\n\nmy_dataset_name = \"demo_equivariant_imaging\"\nmeasurement_dir = DATA_DIR / train_dataset_name / operation\ndeepinv_datasets_path = dinv.datasets.generate_dataset(\n    train_dataset=train_dataset,\n    test_dataset=test_dataset,\n    physics=physics,\n    device=device,\n    save_dir=measurement_dir,\n    train_datapoints=n_images_max,\n    num_workers=num_workers,\n    dataset_filename=str(my_dataset_name),\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)\ntest_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the reconstruction network\n\nAs a reconstruction network, we use an unrolled network (half-quadratic splitting)\nwith a trainable denoising prior based on the DnCNN architecture.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Select the data fidelity term\ndata_fidelity = dinv.optim.L2()\nn_channels = 2  # real + imaginary parts\n\n# If the prior dict value is initialized with a table of length max_iter, then a distinct model is trained for each\n# iteration. For fixed trained model prior across iterations, initialize with a single model.\nprior = PnP(\n    denoiser=dinv.models.DnCNN(\n        in_channels=n_channels,\n        out_channels=n_channels,\n        pretrained=None,\n        train=True,\n        depth=7,\n    ).to(device)\n)\n\n# Unrolled optimization algorithm parameters\nmax_iter = 3  # number of unfolded layers\nlamb = [1.0] * max_iter  # initialization of the regularization parameter\nstepsize = [1.0] * max_iter  # initialization of the step sizes.\nsigma_denoiser = [0.01] * max_iter  # initialization of the denoiser parameters\nparams_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary\n    \"stepsize\": stepsize,\n    \"g_param\": sigma_denoiser,\n    \"lambda\": lamb,\n}\n\ntrainable_params = [\n    \"lambda\",\n    \"stepsize\",\n    \"g_param\",\n]  # define which parameters from 'params_algo' are trainable\n\n# Define the unfolded trainable model.\nmodel = dinv.unfolded.unfolded_builder(\n    \"HQS\",\n    params_algo=params_algo,\n    trainable_params=trainable_params,\n    data_fidelity=data_fidelity,\n    max_iter=max_iter,\n    prior=prior,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the training parameters\nWe choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)\nand the equivariant imaging loss (EI).\nThe EI loss requires a group of transformations to be defined. The forward model [should not be equivariant to\nthese transformations](https://www.jmlr.org/papers/v24/22-0315.html).\nHere we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is\nnot equivariant to rotations (while it is equivariant to translations).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We use a pretrained model to reduce training time. You can get the same results by training from scratch\n      for 150 epochs.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = 1  # choose training epochs\nlearning_rate = 5e-4\nbatch_size = 16 if torch.cuda.is_available() else 1\n\n# choose self-supervised training losses\n# generates 4 random rotations per image in the batch\nlosses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(4))]\n\n# choose optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)\n\n# start with a pretrained model to reduce training time\nurl = online_weights_path() + \"new_demo_ei_ckp_150_v3.pth\"\nckpt = torch.hub.load_state_dict_from_url(\n    url,\n    map_location=lambda storage, loc: storage,\n    file_name=\"new_demo_ei_ckp_150_v3.pth\",\n)\n# load a checkpoint to reduce training time\nmodel.load_state_dict(ckpt[\"state_dict\"])\noptimizer.load_state_dict(ckpt[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the network\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verbose = True  # print training information\nwandb_vis = False  # plot curves and images in Weight&Bias\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True\n)\ntest_dataloader = DataLoader(\n    test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n)\n\ntrain(\n    model=model,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    epochs=epochs,\n    scheduler=scheduler,\n    losses=losses,\n    physics=physics,\n    optimizer=optimizer,\n    device=device,\n    save_path=str(CKPT_DIR / operation),\n    verbose=verbose,\n    wandb_vis=wandb_vis,\n    log_interval=1,\n    eval_interval=1,\n    ckp_interval=10,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the network\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_images = True\nmethod = \"equivariant_imaging\"\n\ntest(\n    model=model,\n    test_dataloader=test_dataloader,\n    physics=physics,\n    device=device,\n    plot_images=plot_images,\n    save_folder=RESULTS_DIR / method / operation,\n    verbose=verbose,\n    wandb_vis=wandb_vis,\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}