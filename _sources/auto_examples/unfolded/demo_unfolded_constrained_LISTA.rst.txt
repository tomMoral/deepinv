
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/unfolded/demo_unfolded_constrained_LISTA.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_unfolded_demo_unfolded_constrained_LISTA.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_unfolded_demo_unfolded_constrained_LISTA.py:


Unfolded Chambolle-Pock for constrained image inpainting
====================================================================================================

Image inpainting consists in solving :math:`y = Ax` where :math:`A` is a mask operator.
This problem can be reformulated as the following minimization problem:

.. math::

    \begin{equation*}
    \underset{x}{\operatorname{min}} \,\, \iota_{\mathcal{B}_2(y, r)}(Ax) + \regname(x)
    \end{equation*}


where :math:`\iota_{\mathcal{B}_2(y, r)}` is the indicator function of the ball of radius :math:`r` centered at
:math:`y` for the :math:`\ell_2` norm, and :math:`\regname` is a regularisation. Recall that the indicator function of
a convex set :math:`\mathcal{C}` is defined as :math:`\iota_{\mathcal{C}}(x) = 0` if :math:`x \in \mathcal{C}` and
:math:`\iota_{\mathcal{C}}(x) = +\infty` otherwise.

In this example, we unfold the Chambolle-Pock algorithm to solve this problem, and learn the thresholding parameters of
a wavelet denoiser in a LISTA fashion.

.. GENERATED FROM PYTHON SOURCE LINES 24-35

.. code-block:: default

    from pathlib import Path
    import torch
    from torch.utils.data import DataLoader
    from torchvision import transforms
    import deepinv as dinv
    from deepinv.utils.demo import load_dataset
    from deepinv.optim.data_fidelity import IndicatorL2
    from deepinv.optim.prior import PnP
    from deepinv.unfolded import unfolded_builder
    from deepinv.training_utils import train, test








.. GENERATED FROM PYTHON SOURCE LINES 36-39

Setup paths for data loading and results.
--------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 39-52

.. code-block:: default


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    DEG_DIR = BASE_DIR / "degradations"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 53-57

Load base image datasets and degradation operators.
--------------------------------------------------------------------------------------------
In this example, we use the CBSD68 dataset for training and the set3c dataset for testing.
We work with images of size 32x32 if no GPU is available, else 128x128.

.. GENERATED FROM PYTHON SOURCE LINES 57-79

.. code-block:: default



    operation = "inpainting"
    train_dataset_name = "CBSD68"
    test_dataset_name = "set3c"
    img_size = 128 if torch.cuda.is_available() else 32

    test_transform = transforms.Compose(
        [transforms.CenterCrop(img_size), transforms.ToTensor()]
    )
    train_transform = transforms.Compose(
        [transforms.RandomCrop(img_size), transforms.ToTensor()]
    )

    train_base_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform=train_transform
    )
    test_base_dataset = load_dataset(
        test_dataset_name, ORIGINAL_DATA_DIR, transform=test_transform
    )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/CBSD68.zip
      0%|          | 0.00/19.8M [00:00<?, ?iB/s]      0%|          | 15.4k/19.8M [00:00<03:37, 90.9kiB/s]      0%|          | 89.1k/19.8M [00:00<00:53, 370kiB/s]       1%|          | 228k/19.8M [00:00<00:27, 713kiB/s]       3%|▎         | 515k/19.8M [00:00<00:13, 1.44MiB/s]      4%|▍         | 794k/19.8M [00:00<00:10, 1.78MiB/s]      6%|▌         | 1.15M/19.8M [00:00<00:08, 2.31MiB/s]      7%|▋         | 1.42M/19.8M [00:00<00:07, 2.43MiB/s]      9%|▊         | 1.70M/19.8M [00:00<00:07, 2.54MiB/s]     10%|█         | 1.99M/19.8M [00:01<00:06, 2.61MiB/s]     12%|█▏        | 2.29M/19.8M [00:01<00:06, 2.73MiB/s]     13%|█▎        | 2.57M/19.8M [00:01<00:06, 2.62MiB/s]     15%|█▍        | 2.88M/19.8M [00:01<00:06, 2.76MiB/s]     16%|█▌        | 3.19M/19.8M [00:01<00:05, 2.85MiB/s]     18%|█▊        | 3.48M/19.8M [00:01<00:05, 2.82MiB/s]     19%|█▉        | 3.77M/19.8M [00:01<00:05, 2.81MiB/s]     20%|██        | 4.05M/19.8M [00:01<00:05, 2.79MiB/s]     22%|██▏       | 4.34M/19.8M [00:01<00:05, 2.82MiB/s]     23%|██▎       | 4.62M/19.8M [00:01<00:05, 2.74MiB/s]     25%|██▍       | 4.91M/19.8M [00:02<00:05, 2.76MiB/s]     26%|██▋       | 5.21M/19.8M [00:02<00:05, 2.82MiB/s]     28%|██▊       | 5.50M/19.8M [00:02<00:05, 2.82MiB/s]     29%|██▉       | 5.81M/19.8M [00:02<00:04, 2.87MiB/s]     31%|███       | 6.11M/19.8M [00:02<00:04, 2.90MiB/s]     32%|███▏      | 6.40M/19.8M [00:02<00:04, 2.79MiB/s]     34%|███▍      | 6.68M/19.8M [00:02<00:04, 2.80MiB/s]     35%|███▌      | 6.98M/19.8M [00:02<00:04, 2.84MiB/s]     37%|███▋      | 7.27M/19.8M [00:02<00:04, 2.84MiB/s]     38%|███▊      | 7.58M/19.8M [00:02<00:04, 2.88MiB/s]     40%|███▉      | 7.88M/19.8M [00:03<00:04, 2.92MiB/s]     41%|████▏     | 8.17M/19.8M [00:03<00:04, 2.83MiB/s]     43%|████▎     | 8.46M/19.8M [00:03<00:04, 2.81MiB/s]     44%|████▍     | 8.75M/19.8M [00:03<00:03, 2.81MiB/s]     46%|████▌     | 9.05M/19.8M [00:03<00:03, 2.85MiB/s]     47%|████▋     | 9.35M/19.8M [00:03<00:03, 2.88MiB/s]     49%|████▉     | 9.66M/19.8M [00:03<00:03, 2.91MiB/s]     50%|█████     | 9.95M/19.8M [00:03<00:03, 2.84MiB/s]     52%|█████▏    | 10.2M/19.8M [00:03<00:03, 2.82MiB/s]     53%|█████▎    | 10.5M/19.8M [00:04<00:03, 2.82MiB/s]     55%|█████▍    | 10.8M/19.8M [00:04<00:03, 2.85MiB/s]     56%|█████▋    | 11.1M/19.8M [00:04<00:03, 2.86MiB/s]     58%|█████▊    | 11.4M/19.8M [00:04<00:02, 2.86MiB/s]     59%|█████▉    | 11.7M/19.8M [00:04<00:02, 2.90MiB/s]     61%|██████    | 12.0M/19.8M [00:04<00:02, 2.80MiB/s]     62%|██████▏   | 12.3M/19.8M [00:04<00:02, 2.82MiB/s]     64%|██████▎   | 12.6M/19.8M [00:04<00:02, 2.78MiB/s]     65%|██████▌   | 12.9M/19.8M [00:04<00:02, 2.84MiB/s]     67%|██████▋   | 13.2M/19.8M [00:04<00:02, 2.85MiB/s]     68%|██████▊   | 13.5M/19.8M [00:05<00:02, 2.88MiB/s]     70%|██████▉   | 13.7M/19.8M [00:05<00:02, 2.87MiB/s]     71%|███████   | 14.0M/19.8M [00:05<00:02, 2.81MiB/s]     73%|███████▎  | 14.3M/19.8M [00:05<00:01, 2.82MiB/s]     74%|███████▍  | 14.6M/19.8M [00:05<00:01, 2.80MiB/s]     75%|███████▌  | 14.9M/19.8M [00:05<00:01, 2.82MiB/s]     77%|███████▋  | 15.2M/19.8M [00:05<00:01, 2.81MiB/s]     78%|███████▊  | 15.5M/19.8M [00:05<00:01, 2.82MiB/s]     80%|███████▉  | 15.8M/19.8M [00:05<00:01, 2.81MiB/s]     81%|████████  | 16.0M/19.8M [00:05<00:01, 2.81MiB/s]     83%|████████▎ | 16.3M/19.8M [00:06<00:01, 2.80MiB/s]     84%|████████▍ | 16.6M/19.8M [00:06<00:01, 2.83MiB/s]     86%|████████▌ | 16.9M/19.8M [00:06<00:01, 2.81MiB/s]     87%|████████▋ | 17.2M/19.8M [00:06<00:00, 2.80MiB/s]     88%|████████▊ | 17.5M/19.8M [00:06<00:00, 2.79MiB/s]     90%|████████▉ | 17.7M/19.8M [00:06<00:00, 2.79MiB/s]     91%|█████████ | 18.0M/19.8M [00:06<00:00, 2.80MiB/s]     93%|█████████▎| 18.3M/19.8M [00:06<00:00, 2.79MiB/s]     94%|█████████▍| 18.6M/19.8M [00:06<00:00, 2.81MiB/s]     96%|█████████▌| 18.9M/19.8M [00:06<00:00, 2.80MiB/s]     97%|█████████▋| 19.2M/19.8M [00:07<00:00, 2.79MiB/s]     98%|█████████▊| 19.4M/19.8M [00:07<00:00, 2.79MiB/s]    100%|█████████▉| 19.7M/19.8M [00:07<00:00, 2.80MiB/s]    100%|██████████| 19.8M/19.8M [00:07<00:00, 2.71MiB/s]
    CBSD68 dataset downloaded in datasets




.. GENERATED FROM PYTHON SOURCE LINES 80-88

Define forward operator and generate dataset
--------------------------------------------------------------------------------------------
We define an inpainting operator that randomly masks pixels with probability 0.5.

A dataset of pairs of measurements and ground truth images is then generated using the
:meth:`dinv.datasets.generate_dataset` function.

Once the dataset is generated, we can load it using the :class:`dinv.datasets.HDF5Dataset` class.

.. GENERATED FROM PYTHON SOURCE LINES 88-131

.. code-block:: default


    n_channels = 3  # 3 for color images, 1 for gray-scale images
    probability_mask = 0.5  # probability to mask pixel

    # Generate inpainting operator
    physics = dinv.physics.Inpainting(
        (n_channels, img_size, img_size), mask=probability_mask, device=device
    )


    # Use parallel dataloader if using a GPU to fasten training,
    # otherwise, as all computes are on CPU, use synchronous data loading.
    num_workers = 4 if torch.cuda.is_available() else 0
    n_images_max = (
        100 if torch.cuda.is_available() else 50
    )  # maximal number of images used for training
    my_dataset_name = "demo_training_inpainting"
    measurement_dir = DATA_DIR / train_dataset_name / operation
    deepinv_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_base_dataset,
        test_dataset=test_base_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)


    train_batch_size = 32 if torch.cuda.is_available() else 3
    test_batch_size = 32 if torch.cuda.is_available() else 3

    train_dataloader = DataLoader(
        train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Computing train measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00,  4.28it/s]    100%|██████████| 1/1 [00:00<00:00,  4.27it/s]
    Computing test measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00, 100.97it/s]
    Dataset has been saved in measurements/CBSD68/inpainting




.. GENERATED FROM PYTHON SOURCE LINES 132-148

Set up the reconstruction network
--------------------------------------------------------
We unfold the Chambolle-Pock algorithm as follows:

     .. math::
         \begin{equation*}
         \begin{aligned}
         u_{k+1} &= \operatorname{prox}_{\sigma d^*}(u_k + \sigma A z_k) \\
         x_{k+1} &= \operatorname{D_{\theta}}(x_k-\tau A^\top u_{k+1}) \\
         z_{k+1} &= 2x_{k+1} -x_k \\
         \end{aligned}
         \end{equation*}

where :math:`\operatorname{D_{\sigma}}` is a wavelet denoiser with thresholding parameters :math:`\sigma`.

The learnable parameters of our network are :math:`\tau` and :math:`\theta`.

.. GENERATED FROM PYTHON SOURCE LINES 148-209

.. code-block:: default


    # Select the data fidelity term
    data_fidelity = IndicatorL2(radius=0.0)

    # Set up the trainable denoising prior; here, the soft-threshold in a wavelet basis.
    # If the prior is initialized with a list of length max_iter,
    # then a distinct weight is trained for each CP iteration.
    # For fixed trained model prior across iterations, initialize with a single model.
    max_iter = 30 if torch.cuda.is_available() else 20  # Number of unrolled iterations
    level = 3
    prior = [
        PnP(denoiser=dinv.models.WaveletPrior(wv="db8", level=level, device=device))
        for i in range(max_iter)
    ]

    # Unrolled optimization algorithm parameters
    lamb = [
        1.0
    ] * max_iter  # initialization of the regularization parameter. A distinct lamb is trained for each iteration.
    stepsize = [
        1.0
    ] * max_iter  # initialization of the stepsizes. A distinct stepsize is trained for each iteration.
    sigma_denoiser = [0.01 * torch.ones(level, 3)] * max_iter

    stepsize_dual = 1.0  # dual stepsize for Chambolle-Pock

    # Define the parameters of the unfolded Primal-Dual Chambolle-Pock algorithm
    # The CP algorithm requires to specify `params_algo`` the linear operator and its adjoint on which splitting is performed.
    # See the documentation of the CP algorithm :meth:`deepinv.optim.optim_iterators.CPIteration` for more details.
    params_algo = {
        "stepsize": stepsize,  # Stepsize for the primal update.
        "g_param": sigma_denoiser,  # prior parameter.
        "lambda": lamb,  # Regularization parameter.
        "stepsize_dual": stepsize_dual,  # The CP algorithm requires a second stepsize ``sigma`` for the dual update.
        "K": physics.A,
        "K_adjoint": physics.A_adjoint,
    }

    # define which parameters from 'params_algo' are trainable
    trainable_params = ["g_param", "stepsize"]


    # Because the CP algorithm uses more than 2 variables, we need to define a custom initialization.
    def custom_init_CP(y, physics):
        x_init = physics.A_adjoint(y)
        u_init = y
        return {"est": (x_init, x_init, u_init)}


    # Define the unfolded trainable model.
    model = unfolded_builder(
        iteration="CP",
        trainable_params=trainable_params,
        params_algo=params_algo.copy(),
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
        g_first=False,
        custom_init=custom_init_CP,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/deepinv/deepinv/deepinv/unfolded/unfolded.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
      [nn.Parameter(torch.tensor(el).to(device)) for el in param_value]




.. GENERATED FROM PYTHON SOURCE LINES 210-222

Train the model
---------------
We train the model using the :meth:`dinv.training_utils.train` function.

We perform supervised learning and use the mean squared error as loss function. This can be easily done using the
:class:`dinv.loss.SupLoss` class.

.. note::

      In this example, we only train for a few epochs to keep the training time short on CPU.
      For a good reconstruction quality, we recommend to train for at least 50 epochs.


.. GENERATED FROM PYTHON SOURCE LINES 222-253

.. code-block:: default


    epochs = 10 if torch.cuda.is_available() else 5  # choose training epochs
    learning_rate = 1e-3

    verbose = True  # print training information
    wandb_vis = False  # plot curves and images in Weight&Bias

    # choose training losses
    losses = dinv.loss.SupLoss(metric=dinv.metric.mse())

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8))

    train(
        model=model,
        train_dataloader=train_dataloader,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        physics=physics,
        optimizer=optimizer,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,
        log_interval=1,
        eval_interval=1,
        ckp_interval=1,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 200 trainable parameters
            23-10-24-10:27:51       [1/5]   loss=5.21e-02   Loss_sup=5.21e-02       Train_psnr_model=15.60  
            23-10-24-10:27:55       [2/5]   loss=2.57e-02   Loss_sup=2.57e-02       Train_psnr_model=19.47  
            23-10-24-10:27:59       [3/5]   loss=1.57e-02   Loss_sup=1.57e-02       Train_psnr_model=21.36  
            23-10-24-10:28:03       [4/5]   loss=1.20e-02   Loss_sup=1.20e-02       Train_psnr_model=22.46  
            23-10-24-10:28:07       [5/5]   loss=9.85e-03   Loss_sup=9.85e-03       Train_psnr_model=23.23  

    BaseUnfold(
      (fixed_point): FixedPoint(
        (iterator): CPIteration(
          (f_step): fStepCP()
          (g_step): gStepCP()
        )
      )
      (init_params_algo): ParameterDict(
          (K): Object of type: list
          (K_adjoint): Object of type: list
          (beta): Object of type: list
          (g_param): Object of type: ParameterList
          (lambda): Object of type: list
          (stepsize): Object of type: ParameterList
          (stepsize_dual): Object of type: list
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size 3x3]
            (1): Parameter containing: [torch.float32 of size 3x3]
            (2): Parameter containing: [torch.float32 of size 3x3]
            (3): Parameter containing: [torch.float32 of size 3x3]
            (4): Parameter containing: [torch.float32 of size 3x3]
            (5): Parameter containing: [torch.float32 of size 3x3]
            (6): Parameter containing: [torch.float32 of size 3x3]
            (7): Parameter containing: [torch.float32 of size 3x3]
            (8): Parameter containing: [torch.float32 of size 3x3]
            (9): Parameter containing: [torch.float32 of size 3x3]
            (10): Parameter containing: [torch.float32 of size 3x3]
            (11): Parameter containing: [torch.float32 of size 3x3]
            (12): Parameter containing: [torch.float32 of size 3x3]
            (13): Parameter containing: [torch.float32 of size 3x3]
            (14): Parameter containing: [torch.float32 of size 3x3]
            (15): Parameter containing: [torch.float32 of size 3x3]
            (16): Parameter containing: [torch.float32 of size 3x3]
            (17): Parameter containing: [torch.float32 of size 3x3]
            (18): Parameter containing: [torch.float32 of size 3x3]
            (19): Parameter containing: [torch.float32 of size 3x3]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
            (5): Parameter containing: [torch.float32 of size ]
            (6): Parameter containing: [torch.float32 of size ]
            (7): Parameter containing: [torch.float32 of size ]
            (8): Parameter containing: [torch.float32 of size ]
            (9): Parameter containing: [torch.float32 of size ]
            (10): Parameter containing: [torch.float32 of size ]
            (11): Parameter containing: [torch.float32 of size ]
            (12): Parameter containing: [torch.float32 of size ]
            (13): Parameter containing: [torch.float32 of size ]
            (14): Parameter containing: [torch.float32 of size ]
            (15): Parameter containing: [torch.float32 of size ]
            (16): Parameter containing: [torch.float32 of size ]
            (17): Parameter containing: [torch.float32 of size ]
            (18): Parameter containing: [torch.float32 of size ]
            (19): Parameter containing: [torch.float32 of size ]
        )
      )
      (params_algo): ParameterDict(
          (K): Object of type: list
          (K_adjoint): Object of type: list
          (beta): Object of type: list
          (g_param): Object of type: ParameterList
          (lambda): Object of type: list
          (stepsize): Object of type: ParameterList
          (stepsize_dual): Object of type: list
        (g_param): ParameterList(
            (0): Parameter containing: [torch.float32 of size 3x3]
            (1): Parameter containing: [torch.float32 of size 3x3]
            (2): Parameter containing: [torch.float32 of size 3x3]
            (3): Parameter containing: [torch.float32 of size 3x3]
            (4): Parameter containing: [torch.float32 of size 3x3]
            (5): Parameter containing: [torch.float32 of size 3x3]
            (6): Parameter containing: [torch.float32 of size 3x3]
            (7): Parameter containing: [torch.float32 of size 3x3]
            (8): Parameter containing: [torch.float32 of size 3x3]
            (9): Parameter containing: [torch.float32 of size 3x3]
            (10): Parameter containing: [torch.float32 of size 3x3]
            (11): Parameter containing: [torch.float32 of size 3x3]
            (12): Parameter containing: [torch.float32 of size 3x3]
            (13): Parameter containing: [torch.float32 of size 3x3]
            (14): Parameter containing: [torch.float32 of size 3x3]
            (15): Parameter containing: [torch.float32 of size 3x3]
            (16): Parameter containing: [torch.float32 of size 3x3]
            (17): Parameter containing: [torch.float32 of size 3x3]
            (18): Parameter containing: [torch.float32 of size 3x3]
            (19): Parameter containing: [torch.float32 of size 3x3]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
            (5): Parameter containing: [torch.float32 of size ]
            (6): Parameter containing: [torch.float32 of size ]
            (7): Parameter containing: [torch.float32 of size ]
            (8): Parameter containing: [torch.float32 of size ]
            (9): Parameter containing: [torch.float32 of size ]
            (10): Parameter containing: [torch.float32 of size ]
            (11): Parameter containing: [torch.float32 of size ]
            (12): Parameter containing: [torch.float32 of size ]
            (13): Parameter containing: [torch.float32 of size ]
            (14): Parameter containing: [torch.float32 of size ]
            (15): Parameter containing: [torch.float32 of size ]
            (16): Parameter containing: [torch.float32 of size ]
            (17): Parameter containing: [torch.float32 of size ]
            (18): Parameter containing: [torch.float32 of size ]
            (19): Parameter containing: [torch.float32 of size ]
        )
      )
      (prior): ModuleList(
        (0-19): 20 x PnP(
          (denoiser): WaveletPrior(
            (dwt): DWTForward()
            (iwt): DWTInverse()
          )
        )
      )
      (data_fidelity): ModuleList(
        (0): IndicatorL2()
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 254-259

Test the network
--------------------------------------------
We can now test the trained network using the :meth:`dinv.training_utils.test` function.

The testing function will compute test_psnr metrics and plot and save the results.

.. GENERATED FROM PYTHON SOURCE LINES 259-274

.. code-block:: default


    plot_images = True
    method = "artifact_removal"

    test_psnr, test_std_psnr, init_psnr, init_std_psnr = test(
        model=model,
        test_dataloader=test_dataloader,
        physics=physics,
        device=device,
        plot_images=plot_images,
        save_folder=RESULTS_DIR / method / operation / test_dataset_name,
        verbose=verbose,
        wandb_vis=wandb_vis,  # training vialisations can be done in Weight&Bias
    )




.. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_unfolded_constrained_LISTA_001.png
   :alt: Input, Linear, Recons., GT
   :srcset: /auto_examples/unfolded/images/sphx_glr_demo_unfolded_constrained_LISTA_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Processing data of operator 1 out of 1
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:03<00:00,  3.91s/it]    100%|██████████| 1/1 [00:03<00:00,  3.91s/it]
    Test PSNR: Linear rec.: 6.66+-0.00 dB | Model: 16.32+-0.00 dB. 




.. GENERATED FROM PYTHON SOURCE LINES 275-278

Saving the model
----------------
We can save the trained model following the standard PyTorch procedure.

.. GENERATED FROM PYTHON SOURCE LINES 278-282

.. code-block:: default


    # Save the model
    torch.save(model.state_dict(), CKPT_DIR / operation / "model.pth")








.. GENERATED FROM PYTHON SOURCE LINES 283-287

Loading the model
-----------------
Similarly, we can load our trained unfolded architecture following the standard PyTorch procedure.
To check that the loading is performed correctly, we use new variables for the initialization of the model.

.. GENERATED FROM PYTHON SOURCE LINES 287-347

.. code-block:: default


    # Set up the trainable denoising prior; here, the soft-threshold in a wavelet basis.
    level = 3
    model_spec = {
        "name": "waveletprior",
        "args": {"wv": "db8", "level": level, "device": device},
    }
    # If the prior is initialized with a list of length max_iter,
    # then a distinct weight is trained for each PGD iteration.
    # For fixed trained model prior across iterations, initialize with a single model.
    max_iter = 30 if torch.cuda.is_available() else 20  # Number of unrolled iterations
    prior_new = [
        PnP(denoiser=dinv.models.WaveletPrior(wv="db8", level=level, device=device))
        for i in range(max_iter)
    ]

    # Unrolled optimization algorithm parameters
    lamb = [
        1.0
    ] * max_iter  # initialization of the regularization parameter. A distinct lamb is trained for each iteration.
    stepsize = [
        1.0
    ] * max_iter  # initialization of the stepsizes. A distinct stepsize is trained for each iteration.
    sigma_denoiser = [0.01 * torch.ones(level, 3)] * max_iter

    stepsize_dual = 1.0  # stepsize for Chambolle-Pock

    params_algo_new = {
        "stepsize": stepsize,
        "g_param": sigma_denoiser,
        "lambda": lamb,
        "stepsize_dual": stepsize_dual,
        "K": physics.A,
        "K_adjoint": physics.A_adjoint,
    }

    model_new = unfolded_builder(
        "CP",
        trainable_params=trainable_params,
        params_algo=params_algo_new,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior_new,
        g_first=False,
        custom_init=custom_init_CP,
    )
    model_new.load_state_dict(torch.load(CKPT_DIR / operation / "model.pth"))
    model_new.eval()

    # Test the model and check that the results are the same as before saving
    test_psnr, test_std_psnr, init_psnr, init_std_psnr = test(
        model=model_new,
        test_dataloader=test_dataloader,
        physics=physics,
        device=device,
        plot_images=plot_images,
        save_folder=RESULTS_DIR / method / operation / test_dataset_name,
        verbose=verbose,
        wandb_vis=wandb_vis,
    )



.. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_unfolded_constrained_LISTA_002.png
   :alt: Input, Linear, Recons., GT
   :srcset: /auto_examples/unfolded/images/sphx_glr_demo_unfolded_constrained_LISTA_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Processing data of operator 1 out of 1
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:03<00:00,  3.92s/it]    100%|██████████| 1/1 [00:03<00:00,  3.92s/it]
    Test PSNR: Linear rec.: 6.66+-0.00 dB | Model: 16.32+-0.00 dB. 





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 39.569 seconds)


.. _sphx_glr_download_auto_examples_unfolded_demo_unfolded_constrained_LISTA.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_unfolded_constrained_LISTA.py <demo_unfolded_constrained_LISTA.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_unfolded_constrained_LISTA.ipynb <demo_unfolded_constrained_LISTA.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
