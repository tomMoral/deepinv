
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/basics/demo_train_inpainting.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_basics_demo_train_inpainting.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_basics_demo_train_inpainting.py:


Training a reconstruction network.
====================================================================================================

This example shows how to train a simple reconstruction network for an image
inpainting inverse problem.

.. GENERATED FROM PYTHON SOURCE LINES 9-18

.. code-block:: default


    import deepinv as dinv
    from torch.utils.data import DataLoader
    import torch
    from pathlib import Path
    from torchvision import transforms
    from deepinv.utils.demo import load_dataset
    from deepinv.training_utils import train, test








.. GENERATED FROM PYTHON SOURCE LINES 19-22

Setup paths for data loading and results.
--------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 22-35

.. code-block:: default


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    DEG_DIR = BASE_DIR / "degradations"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 36-40

Load base image datasets and degradation operators.
--------------------------------------------------------------------------------------------
In this example, we use the CBSD68 dataset for training and the set3c dataset for testing.
We work with images of size 32x32 if no GPU is available, else 128x128.

.. GENERATED FROM PYTHON SOURCE LINES 40-57

.. code-block:: default



    operation = "inpainting"
    train_dataset_name = "CBSD68"
    test_dataset_name = "set3c"
    img_size = 128 if torch.cuda.is_available() else 32

    test_transform = transforms.Compose(
        [transforms.CenterCrop(img_size), transforms.ToTensor()]
    )
    train_transform = transforms.Compose(
        [transforms.RandomCrop(img_size), transforms.ToTensor()]
    )

    train_dataset = load_dataset(train_dataset_name, ORIGINAL_DATA_DIR, train_transform)
    test_dataset = load_dataset(test_dataset_name, ORIGINAL_DATA_DIR, test_transform)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/CBSD68.zip
      0%|          | 0.00/19.8M [00:00<?, ?iB/s]      0%|          | 93.2k/19.8M [00:00<00:37, 526kiB/s]      2%|▏         | 388k/19.8M [00:00<00:12, 1.60MiB/s]      3%|▎         | 683k/19.8M [00:00<00:08, 2.13MiB/s]      5%|▌         | 1.01M/19.8M [00:00<00:07, 2.38MiB/s]      6%|▋         | 1.27M/19.8M [00:00<00:08, 2.27MiB/s]      8%|▊         | 1.64M/19.8M [00:00<00:06, 2.69MiB/s]     10%|▉         | 1.94M/19.8M [00:00<00:06, 2.79MiB/s]     12%|█▏        | 2.28M/19.8M [00:00<00:06, 2.80MiB/s]     13%|█▎        | 2.57M/19.8M [00:01<00:06, 2.61MiB/s]     15%|█▍        | 2.94M/19.8M [00:01<00:05, 2.89MiB/s]     16%|█▋        | 3.24M/19.8M [00:01<00:05, 2.92MiB/s]     18%|█▊        | 3.53M/19.8M [00:01<00:05, 2.90MiB/s]     19%|█▉        | 3.83M/19.8M [00:01<00:06, 2.61MiB/s]     21%|██        | 4.12M/19.8M [00:01<00:05, 2.67MiB/s]     22%|██▏       | 4.43M/19.8M [00:01<00:05, 2.78MiB/s]     24%|██▍       | 4.72M/19.8M [00:01<00:05, 2.80MiB/s]     25%|██▌       | 5.02M/19.8M [00:01<00:05, 2.84MiB/s]     27%|██▋       | 5.31M/19.8M [00:02<00:05, 2.87MiB/s]     28%|██▊       | 5.60M/19.8M [00:02<00:05, 2.74MiB/s]     30%|██▉       | 5.89M/19.8M [00:02<00:05, 2.77MiB/s]     31%|███▏      | 6.19M/19.8M [00:02<00:04, 2.84MiB/s]     33%|███▎      | 6.49M/19.8M [00:02<00:04, 2.88MiB/s]     34%|███▍      | 6.78M/19.8M [00:02<00:04, 2.85MiB/s]     36%|███▌      | 7.08M/19.8M [00:02<00:04, 2.89MiB/s]     37%|███▋      | 7.37M/19.8M [00:02<00:04, 2.77MiB/s]     39%|███▉      | 7.66M/19.8M [00:02<00:04, 2.81MiB/s]     40%|████      | 7.96M/19.8M [00:02<00:04, 2.84MiB/s]     42%|████▏     | 8.25M/19.8M [00:03<00:04, 2.87MiB/s]     43%|████▎     | 8.55M/19.8M [00:03<00:03, 2.86MiB/s]     45%|████▍     | 8.85M/19.8M [00:03<00:03, 2.90MiB/s]     46%|████▋     | 9.14M/19.8M [00:03<00:03, 2.90MiB/s]     48%|████▊     | 9.43M/19.8M [00:03<00:03, 2.78MiB/s]     49%|████▉     | 9.72M/19.8M [00:03<00:03, 2.81MiB/s]     51%|█████     | 10.0M/19.8M [00:03<00:03, 2.87MiB/s]     52%|█████▏    | 10.3M/19.8M [00:03<00:03, 2.87MiB/s]     54%|█████▎    | 10.6M/19.8M [00:03<00:03, 2.89MiB/s]     55%|█████▌    | 10.9M/19.8M [00:03<00:03, 2.86MiB/s]     57%|█████▋    | 11.2M/19.8M [00:04<00:03, 2.76MiB/s]     58%|█████▊    | 11.5M/19.8M [00:04<00:02, 2.79MiB/s]     60%|█████▉    | 11.8M/19.8M [00:04<00:02, 2.83MiB/s]     61%|██████    | 12.1M/19.8M [00:04<00:02, 2.89MiB/s]     63%|██████▎   | 12.4M/19.8M [00:04<00:02, 2.89MiB/s]     64%|██████▍   | 12.7M/19.8M [00:04<00:02, 2.86MiB/s]     66%|██████▌   | 13.0M/19.8M [00:04<00:02, 2.77MiB/s]     67%|██████▋   | 13.3M/19.8M [00:04<00:02, 2.83MiB/s]     69%|██████▊   | 13.5M/19.8M [00:04<00:02, 2.82MiB/s]     70%|███████   | 13.8M/19.8M [00:05<00:02, 2.85MiB/s]     72%|███████▏  | 14.1M/19.8M [00:05<00:01, 2.88MiB/s]     73%|███████▎  | 14.4M/19.8M [00:05<00:01, 2.90MiB/s]     75%|███████▍  | 14.7M/19.8M [00:05<00:01, 2.87MiB/s]     76%|███████▌  | 15.0M/19.8M [00:05<00:01, 2.78MiB/s]     77%|███████▋  | 15.3M/19.8M [00:05<00:01, 2.79MiB/s]     79%|███████▉  | 15.6M/19.8M [00:05<00:01, 2.77MiB/s]     80%|████████  | 15.9M/19.8M [00:05<00:01, 2.81MiB/s]     82%|████████▏ | 16.2M/19.8M [00:05<00:01, 2.86MiB/s]     83%|████████▎ | 16.5M/19.8M [00:05<00:01, 2.88MiB/s]     85%|████████▍ | 16.8M/19.8M [00:06<00:01, 2.82MiB/s]     86%|████████▋ | 17.1M/19.8M [00:06<00:00, 2.82MiB/s]     88%|████████▊ | 17.3M/19.8M [00:06<00:00, 2.79MiB/s]     89%|████████▉ | 17.6M/19.8M [00:06<00:00, 2.84MiB/s]     91%|█████████ | 17.9M/19.8M [00:06<00:00, 2.86MiB/s]     92%|█████████▏| 18.3M/19.8M [00:06<00:00, 2.92MiB/s]     94%|█████████▍| 18.5M/19.8M [00:06<00:00, 2.80MiB/s]     95%|█████████▌| 18.8M/19.8M [00:06<00:00, 2.82MiB/s]     97%|█████████▋| 19.1M/19.8M [00:06<00:00, 2.81MiB/s]     98%|█████████▊| 19.4M/19.8M [00:07<00:00, 2.86MiB/s]    100%|█████████▉| 19.7M/19.8M [00:07<00:00, 2.88MiB/s]    100%|██████████| 19.8M/19.8M [00:07<00:00, 2.78MiB/s]
    CBSD68 dataset downloaded in datasets




.. GENERATED FROM PYTHON SOURCE LINES 58-66

Define forward operator and generate dataset
--------------------------------------------------------------------------------------------
We define an inpainting operator that randomly masks pixels with probability 0.5.

A dataset of pairs of measurements and ground truth images is then generated using the
:meth:`deepinv.datasets.generate_dataset` function.

Once the dataset is generated, we can load it using the :class:`deepinv.datasets.HDF5Dataset` class.

.. GENERATED FROM PYTHON SOURCE LINES 66-109

.. code-block:: default


    n_channels = 3  # 3 for color images, 1 for gray-scale images
    probability_mask = 0.5  # probability to mask pixel

    # Generate inpainting operator
    physics = dinv.physics.Inpainting(
        (n_channels, img_size, img_size), mask=probability_mask, device=device
    )


    # Use parallel dataloader if using a GPU to fasten training,
    # otherwise, as all computes are on CPU, use synchronous data loading.
    num_workers = 4 if torch.cuda.is_available() else 0
    n_images_max = (
        1000 if torch.cuda.is_available() else 50
    )  # maximal number of images used for training
    my_dataset_name = "demo_training_inpainting"
    measurement_dir = DATA_DIR / train_dataset_name / operation
    deepinv_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)


    train_batch_size = 32 if torch.cuda.is_available() else 1
    test_batch_size = 32 if torch.cuda.is_available() else 1

    train_dataloader = DataLoader(
        train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Computing train measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00,  4.25it/s]    100%|██████████| 1/1 [00:00<00:00,  4.24it/s]
    Computing test measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00, 101.07it/s]
    Dataset has been saved in measurements/CBSD68/inpainting




.. GENERATED FROM PYTHON SOURCE LINES 110-120

Set up the reconstruction network
--------------------------------------------------------
We use a simple inversion architecture of the form

     .. math::

              f_{\theta}(y) = \phi_{\theta}(A^{\top}(y))

where the linear reconstruction :math:`A^{\top}y` is post-processed by a U-Net network :math:`\phi_{\theta}` is a
neural network with trainable parameters :math:`\theta`.

.. GENERATED FROM PYTHON SOURCE LINES 120-130

.. code-block:: default



    # choose backbone model
    backbone = dinv.models.UNet(
        in_channels=3, out_channels=3, scales=3, batch_norm=False
    ).to(device)

    # choose a reconstruction architecture
    model = dinv.models.ArtifactRemoval(backbone)








.. GENERATED FROM PYTHON SOURCE LINES 131-143

Train the model
----------------------------------------------------------------------------------------
We train the model using the :meth:`deepinv.training_utils.train` function.

We perform supervised learning and use the mean squared error as loss function. This can be easily done using the
:class:`deepinv.loss.SupLoss` class.

.. note::

      In this example, we only train for a few epochs to keep the training time short.
      For a good reconstruction quality, we recommend to train for at least 100 epochs.


.. GENERATED FROM PYTHON SOURCE LINES 143-174

.. code-block:: default


    epochs = 4  # choose training epochs
    learning_rate = 5e-4

    verbose = True  # print training information
    wandb_vis = False  # plot curves and images in Weight&Bias

    # choose training losses
    losses = dinv.loss.SupLoss(metric=dinv.metric.mse())

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8))

    train(
        model=model,
        train_dataloader=train_dataloader,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        physics=physics,
        optimizer=optimizer,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,
        log_interval=2,
        eval_interval=2,
        ckp_interval=2,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 2067779 trainable parameters
            23-10-24-10:10:03       [2/4]   loss=3.95e-02   Loss_sup=3.95e-02       Train_psnr_model=15.90  
            23-10-24-10:10:06       [4/4]   loss=9.59e-03   Loss_sup=9.59e-03       Train_psnr_model=21.33  

    ArtifactRemoval(
      (backbone_net): UNet(
        (Maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (Conv1): Sequential(
          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): ReLU(inplace=True)
        )
        (Conv2): Sequential(
          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): ReLU(inplace=True)
        )
        (Conv3): Sequential(
          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): ReLU(inplace=True)
        )
        (Up3): Sequential(
          (0): Upsample(scale_factor=2.0, mode='nearest')
          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): ReLU(inplace=True)
        )
        (Up_conv3): Sequential(
          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): ReLU(inplace=True)
        )
        (Up2): Sequential(
          (0): Upsample(scale_factor=2.0, mode='nearest')
          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): ReLU(inplace=True)
        )
        (Up_conv2): Sequential(
          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): ReLU(inplace=True)
        )
        (Conv_1x1): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 175-180

Test the network
--------------------------------------------
We can now test the trained network using the :meth:`deepinv.test` function.

The testing function will compute test_psnr metrics and plot and save the results.

.. GENERATED FROM PYTHON SOURCE LINES 180-194

.. code-block:: default


    plot_images = True
    method = "artifact_removal"

    test_psnr, test_std_psnr, init_psnr, init_std_psnr = test(
        model=model,
        test_dataloader=test_dataloader,
        physics=physics,
        device=device,
        plot_images=plot_images,
        save_folder=RESULTS_DIR / method / operation / test_dataset_name,
        verbose=verbose,
        wandb_vis=wandb_vis,
    )



.. image-sg:: /auto_examples/basics/images/sphx_glr_demo_train_inpainting_001.png
   :alt: Input, Linear, Recons., GT
   :srcset: /auto_examples/basics/images/sphx_glr_demo_train_inpainting_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Processing data of operator 1 out of 1
      0%|          | 0/3 [00:00<?, ?it/s]     33%|███▎      | 1/3 [00:01<00:02,  1.43s/it]    100%|██████████| 3/3 [00:01<00:00,  1.99it/s]
    Test PSNR: Linear rec.: 6.66+-1.18 dB | Model: 16.88+-0.73 dB. 





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 16.678 seconds)


.. _sphx_glr_download_auto_examples_basics_demo_train_inpainting.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_train_inpainting.py <demo_train_inpainting.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_train_inpainting.ipynb <demo_train_inpainting.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
